{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Aprendizagem sobre textos com tokenização"
      ],
      "metadata": {
        "id": "Ga_wJtvWQPN3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Base de dados:\n",
        "\n",
        "IMDb (Internet Movie Database)\n",
        "\n",
        "É um conjunto de avaliações de filmes coletadas do site IMDb, uma das maiores bases de dados online de informações sobre filmes, programas de televisão, atores, e mais relacionados à indústria do entretenimento. O conjunto de dados consiste em avaliações de filmes feitas por usuários do IMDb, onde cada avaliação é rotulada como positiva ou negativa com base na pontuação atribuída pelo usuário ao filme. Essas avaliações são frequentemente usadas em tarefas de análise de sentimentos ou de classificação de texto para identificar se uma avaliação é positiva ou negativa em relação a um filme específico."
      ],
      "metadata": {
        "id": "ytkMPCZsoMnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cenário:\n",
        "\n",
        "Treinar um modelo de rede neural para classificar avaliações de filmes como positivas ou negativas, com base nos textos das avaliações.\n",
        "\n",
        "*   Os dados de entrada são os textos das avaliações dos filmes;\n",
        "*   A variável de saída (Y) é a classificação da avaliação como positiva (like) ou negativa (dislike);\n",
        "*  O modelo é treinado com um conjunto de dados rotulados, onde cada avaliação está associada a uma classificação (like or dislike);\n",
        "*   O objetivo do modelo é aprender a mapear os textos das avaliações para as classificações correspondentes, possibilitando classificar novas avaliações corretamente."
      ],
      "metadata": {
        "id": "sopQ4x03oUyL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Y:\n",
        "\n",
        "*   Dislike = 0\n",
        "*   Like = 1"
      ],
      "metadata": {
        "id": "pwn56VXYoaRa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Técnica de tokenização:\n",
        "\n",
        "É o processo de dividir um texto em partes menores. Esses tokens podem ser palavras individuais, números, pontuações ou qualquer outra unidade significativa de um texto.\n",
        "\n",
        "Na base escolhida, a tokenização é realizada nos textos das avaliações dos filmes. Cada palavra em uma avaliação é convertida em um número inteiro único, que representa o índice dessa palavra no vocabulário.\n",
        "\n",
        "O Keras possui a função **imdb.load_data()** que carrega o conjunto de dados pré-tokenizado, onde as palavras já são convertidas em índices."
      ],
      "metadata": {
        "id": "rqNiaJpCod6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Protocolo experimental da Rede Neural -> Tokenização implícita: imdb.load_data()"
      ],
      "metadata": {
        "id": "tj6eO8dmsKsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# parametros\n",
        "max_features = 5000  # numero max de palavras consideradas\n",
        "maxlen = 300  # tamanho max de avaliação\n",
        "embedding_size = 128  # tamanho do vetor de embedding -> representação numérica de token\n",
        "lstm_units = 64  # numero de unidades LSTM -> célula rede neural\n",
        "\n",
        "# carregando base de dados\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# padronizando sequencias\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "# criando modelo\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
        "model.add(LSTM(lstm_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilando modelo\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# treinando modelo\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=3, validation_data=(X_test, y_test))\n",
        "\n",
        "# avaliando modelo\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'acurácia: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzeOO33FQe3G",
        "outputId": "9babe600-0e0b-4bc2-d2bb-3c757849bf33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "391/391 [==============================] - 181s 458ms/step - loss: 0.4265 - accuracy: 0.8055 - val_loss: 0.3517 - val_accuracy: 0.8560\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 180s 462ms/step - loss: 0.2703 - accuracy: 0.8929 - val_loss: 0.3075 - val_accuracy: 0.8759\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 180s 460ms/step - loss: 0.2249 - accuracy: 0.9131 - val_loss: 0.3189 - val_accuracy: 0.8740\n",
            "782/782 [==============================] - 37s 48ms/step - loss: 0.3189 - accuracy: 0.8740\n",
            "Acurácia do modelo: 0.8740000128746033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Protocolo experimental da Rede Neural -> Tokenização manual"
      ],
      "metadata": {
        "id": "m794bGZ2sl7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# parametros\n",
        "max_features = 5000  # numero max de palavras consideradas\n",
        "maxlen = 300  # tamanho max de avaliação\n",
        "embedding_size = 128  # tamanho do vetor de embedding -> representação numérica de token\n",
        "lstm_units = 64  # numero de unidades LSTM -> célula rede neural\n",
        "\n",
        "# carregando base de dados\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "# tokenização manual\n",
        "word_index = imdb.get_word_index()\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
        "\n",
        "# decodificando uma avaliação\n",
        "print(decode_review(X_train[0]))\n",
        "\n",
        "# padronizando sequencias\n",
        "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
        "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
        "\n",
        "# criando modelo\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
        "model.add(LSTM(lstm_units))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "# compilando modelo\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# treinando modelo\n",
        "model.fit(X_train, y_train, batch_size=64, epochs=3, validation_data=(X_test, y_test))\n",
        "\n",
        "# avaliando modelo\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f'acurácia: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eCsDzSfBlJXT",
        "outputId": "aa3c2c46-dbe7-4ce0-ee1d-9badcafa7d6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1641221/1641221 [==============================] - 0s 0us/step\n",
            "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly ? was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little ? that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big ? for the whole film but these children are amazing and should be ? for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was ? with us all\n",
            "Epoch 1/3\n",
            "391/391 [==============================] - 182s 460ms/step - loss: 0.4444 - accuracy: 0.7887 - val_loss: 0.3702 - val_accuracy: 0.8443\n",
            "Epoch 2/3\n",
            "391/391 [==============================] - 180s 461ms/step - loss: 0.2997 - accuracy: 0.8784 - val_loss: 0.3748 - val_accuracy: 0.8318\n",
            "Epoch 3/3\n",
            "391/391 [==============================] - 181s 463ms/step - loss: 0.2402 - accuracy: 0.9054 - val_loss: 0.3102 - val_accuracy: 0.8736\n",
            "782/782 [==============================] - 39s 50ms/step - loss: 0.3102 - accuracy: 0.8736\n",
            "Acurácia do modelo: 0.8736000061035156\n"
          ]
        }
      ]
    }
  ]
}